---
layout: page
title: Thursday, February 21st
sidebar: true
---

## SalesEngine Submission Notes

### Submissions

All project submissions are available here:

https://github.com/gSchool/submissions/blob/master/projects/sales_engine.markdown

### Test Coverage

Your test suite should be generating a report in `coverage/index.html` with your test coverage percentage every time the suite is run.

### Code Cleanliness

Make sure you've added the cane and reek rake tasks as posted in chat yesterday:

{% terminal %}
$ git remote add upstream git://github.com/gSchool/sales_engine.git
$ git pull upstream master
{% endterminal %}

You'll likely get a merge conflict in both your `Gemfile` and your `Rakefile`. You'll need to manually clean those up in your editor, let us know if you need help. Once those conflicts are resolved:

{% terminal %}
$ bundle
$ bundle exec rake sanitation:lines
$ bundle exec rake sanitation:methods
{% endterminal %}

Any reported issues will affect your style scoring -- fix them up if you have time!

## Evaluating SalesEngine

You will peer assess *three different SalesEngine projects* using the instructions below.

### Evaluation Assignments

* Daniel Mee & Raphael Weiner evaluate projects by:
  * Phil Battos & Geoffrey Schorkopf
  * Kyle Suss & John Maddux
  * Laura Steadman & Elaine Tai
* Kyle Suss & John Maddux evaluate projects by:
  * Bradley Sheehan & Kareem Grant
  * Daniel Mee & Raphael Weiner
  * Laura Steadman & Elaine Tai
* Laura Steadman & Elaine Tai evaluate projects by:
  * Phil Battos & Geoffrey Schorkopf
  * Aimee Maher & Blair Anderson
  * Kyle Suss & John Maddux
* Bradley Sheehan & Kareem Grant evaluate projects by:
  * Phil Battos & Geoffrey Schorkopf
  * Jorge Tellez & Ron Rateau
  * Jennifer Eliuk & Josh Mejia
* Aimee Maher & Blair Anderson evaluate projects by:
  * Bradley Sheehan & Kareem Grant
  * Christopher Knight & Danny Garcia
  * Paul Blackwell & James Denman
* Paul Blackwell & James Denman evaluate projects by:
  * Daniel Mee & Raphael Weiner
  * Jorge Tellez & Ron Rateau
  * Jennifer Eliuk & Josh Mejia
* Jennifer Eliuk & Josh Mejia evaluate projects by:
  * Christopher Knight & Danny Garcia
  * Paul Blackwell & James Denman
  * Daniel Mee & Raphael Weiner
* Phil Battos & Geoffrey Schorkopf evaluate projects by:
  * Christopher Knight & Danny Garcia
  * Jennifer Eliuk & Josh Mejia
  * Jorge Tellez & Ron Rateau
* Logan Sears & Chelsea Komlo evaluate projects by:
  * Shane Rogers & Erin Drummond
  * Kyle Suss & John Maddux
  * Paul Blackwell & James Denman
* Christopher Knight & Danny Garcia evaluate projects by:
  * Logan Sears & Chelsea Komlo
  * Bradley Sheehan & Kareem Grant
  * Shane Rogers & Erin Drummond
* Jorge Tellez & Ron Rateau evaluate projects by:
  * Logan Sears & Chelsea Komlo
  * Aimee Maher & Blair Anderson
  * Shane Rogers & Erin Drummond
* Shane Rogers & Erin Drummond evaluate projects by:
  * Aimee Maher & Blair Anderson
  * Logan Sears & Chelsea Komlo
  * Laura Steadman & Elaine Tai

### Starting the Eval

Open http://eval.jumpstartlab.com and begin the evaluation for your first peer pair.

### Setup Folders and Code

* Create a folder on your system named `sales_engine_evaluation_1`
* `cd` into that directory
* Clone the project you're evaluating by substituting the correct URL into this instruction:

{% terminal %}
$ git clone git://github.com/username/sales_engine.git
{% endterminal %}

* Clone the spec harness repository within the same `sales_engine_evaluation` folder:

{% terminal %}
$ git clone git://github.com/gSchool/sales_engine_spec_harness.git
{% endterminal %}

### Correctness - Setup & Run the Spec Harness

Go into the harness directory, install dependencies, and run the harness tests:

{% terminal %}
$ cd sales_engine_spec_harness
$ bundle
$ rake spec
{% endterminal %}

Then record your findings in the evaluation form.

```plain
Correctness
4: All provided tests pass without an error or crash
3: One test failed or crashed
2: Two or three tests failed or crashed
1: More than three tests failed or crashed
0: Program will not run
```

### Effort - Evaluating Extensions

There are individual rake tasks for each set of extensions. Run each of these to see if their code passes the extension tests:

{% terminal %}
$ rake spec:extensions:merchant
$ rake spec:extensions:invoice
$ rake spec:extensions:customer
{% endterminal %}

Use this data in combination with results from the base expectations tests (above) to determine and record the Effort score.

```plain
Effort
5: Program fulfills all Base Expectations and three Extensions
4: Program fulfills all Base Expectations and two Extensions
3: Program fulfills all Base Expectations
2: Program fulfills Base Expectations except for one or two features.
1: Program fulfills some Base Expectations, but more than two features are broken.
0: Program does not fulfill any of the Base Expectations
```

### Testing - Evaluate Test Coverage

Next you'll need to check out their test coverage from within their project:

{% terminal %}
$ cd ../sales_engine
$ bundle
$ rake test
$ open coverage/index.html
{% endterminal %}

Use the code coverage percentage to determine and record the score.

```plain
Testing
4: Testing suite covers >95% of application code
3: Testing suite covers 90-94% of application code
2: Testing suite covers 80-89% of application code
1: Testing suite covers 50-89% of application code
0: Testing suite covers <50% of application code
```

### Style - Evaluate Code Style

Use the automated rake tasks to evaluate the code style from within the project directory:

{% terminal %}
$ bundle
$ bundle exec rake sanitation:all
{% endterminal %}

Count the number of style infractions and determine/record the score.

```plain
Style
4: Source code consistently uses strong code style including lines under 80 characters, methods under 10 lines of code, and correct indentation.
3: Source code uses good code style, but breaks the above criteria in three or fewer spots
2: Source code uses mixed style, with three to six style breaks
1: Source code is generally messy with six to twelve issues
0: Source code is unacceptable, containing more than twelve style issues
```

### Submit the Evaluation

Review all your scores for accuracy, add comments you have, and submit the eval.

### `3.times`

Go back and repeat this process for the other two pairs you're evaluating. It's recommended to start with a totally new directory to hold the second project and a second copy of the spec harness, just to make 100% sure you don't accidentially run the first pair's code during the second eval.

## Contributions

If you have extra time, consider:

1. Dig into your peers' code and do some "refactoring for understanding"
2. Help them improve test coverage

### Setting Up a Fork

You'll have difficulty forking their repository because you already have a fork of the same project with the same name. Instead...

* On the top right corner of any GitHub screen, find the "book with a +" *on and click it
* Give the repository a name, like "sales_engine_refactoring"
* Leave it as public. Move the cursor out of the "repository name" text box *en click "Create Repository"
* In the "Quick Setup" box, copy the SSH url (ex: "git@github.*m:jcasimir/sales_engine_refactoring.git")
* Go to your terminal and change into the peer project you're hacking on
* Add a new remote **with the URL you just copied**:

{% terminal %}
git remote add mine git@github.com:jcasimir/sales_engine_refactoring.git
{% endterminal %}

* Make your code changes and commit them
* Go to your repo page on GitHub
* Look for the "Pull Request" button in the top right
* Submit pull request(s) with comments about your changes

## Afternoon

### Understanding Load Paths

* Why does `require './lib/sales_engine/merchant'` work when I run my code
  but then fails when I run the code from the *sales\_engine\_spec\_harness*?

* Why does my code run but not my tests?

* When do I need to require files?

* What is the best place for requiring external libraries (e.g. 'csv') and gems?

* How can I prevent what happened on this project from happening on the next
  project?


### Rake Tasks

* What is Rake?
* Defining a *task* with the Rake Domain Specific Language (DSL)
* Defining a *task* with a RakeTask Object

### Testing

* Running your test suite whenver you make a change

Update your *Gemfile*:

```ruby
gem 'guard-minitest'
```

{% terminal %}
$ bundle
$ guard init
{% endterminal %}

Open up your *Guardfile*:

```ruby
guard 'minitest' do
  # with Minitest::Unit
  watch(%r|^test/(.*)\/?(.*)_test\.rb|)
  watch(%r|^lib/(.*)([^/]+)\.rb|)     { |m| "test/#{m[1]}test_#{m[2]}.rb" }
  watch(%r|^test/test_helper\.rb|)    { "test" }
end
```

* Fixing a slow test suite through stubs

```ruby
gem 'rspec-mocks'
```

{% terminal %}
$ bundle
{% endterminal %}

```ruby
require 'minitest/autorun'
require 'rspec/mocks'

MiniTest::Unit::TestCase.add_setup_hook do |test_case|
  RSpec::Mocks.setup(test_case)
end

MiniTest::Unit::TestCase.add_teardown_hook do |test_case|
  begin
    RSpec::Mocks.verify
  ensure
    RSpec::Mocks.teardown
  end
end
```
